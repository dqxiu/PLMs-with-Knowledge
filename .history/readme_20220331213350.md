
# Paper Notes on Pretrain Language Models with Factual Knowledge


<!-- omit in toc -->
## Contents


- [Paper Notes on Pretrain Language Models with Factual Knowledge](#paper-notes-on-pretrain-language-models-with-factual-knowledge)
  - [Introduction](#introduction)
    - [Keywords Convention](#keywords-convention)
  - [Papers](#papers)
    - [Probing](#probing)
    - [Knowledge Retrieving](#knowledge-retrieving)
    - [Knowledge Injection](#knowledge-injection)
    - [Model Editing](#model-editing)




## Introduction

This is a paper list about **PLMs & Knowledge**. 

### Keywords Convention

![](https://img.shields.io/badge/FaE-DCE7F1) abbreviation 

![](https://img.shields.io/badge/Efficient-EAD8D9) key features

![](https://img.shields.io/badge/QA-D8D0E1) main task

![](https://img.shields.io/badge/20220331-FAEFCA) editing info

## Papers


### Probing

1. LAMA
2. ParaREL

### Knowledge Retrieving

1. REALM

### Knowledge Injection

1. EaE
2. FaE
3. OPQL
   
### Model Editing
This section contains the pilot works that might contributes to the prevalence of model editiing paradigm.
1. **Editable Neural Networks** ICLR 2020. ![](https://img.shields.io/badge/Editable_training-DCE7F1) 

   *Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko*.  [[pdf](https://openreview.net/pdf?id=HJedXaEtvS)], [[project](https://github.com/xtinkt/editable)],  2020.7
   ![](https://img.shields.io/badge/image_classification-D8D0E1) ![](https://img.shields.io/badge/machine_translation-D8D0E1)

   Related work:
   - re-train on the original dataset augmented with new samples (expensive)
   - use a manual cache (e.g. lookup table) that overrules model predictions on problematic samples (can't generalize to semantic equivalent inputs)
   Method:
   Editable Training (employ meta-learning techniques to ensure that mistakes can be corrected without harming its overall performance)
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/editable_train.png" width="800" alt="webhooks">


2. **Fact-based Text Editing** ACL 2020. ![](https://img.shields.io/badge/Fact_based_text_editing-DCE7F1) 
   
   *Hayate Iso, Chao Qiao, Hang Li*.  [[pdf](https://aclanthology.org/2020.acl-main.17.pdf)], [[project](https://github.com/isomap/factedit)],  2020.7
   ![](https://img.shields.io/badge/edit_draft_text_editing-D8D0E1) 


   Main cons.:
   - propose a new dataset (task): transforms a draft text into a revised text based on given triples
   - new model, FACTEDITOR. The model consists of three components, a buffer for storing the draft text and its representations, a stream for storing the revised text and its representations, and a memory for storing the triples and their representations
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/fte.png" width="300" alt="webhooks">


 2. Modifying Memories in Transformer Models
   Explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.
  - updating stale knowledge
  - protecting privacy
  - eliminating unintended biases (debias)
  Baseline
Retraining the model on modified training set
Fine-tuning on modified facts
Fine-tuning on a mixture of modified and unmodified batches
Method
 Constrained fine-tuning on supporting evidences for modified facts


 3. Editing Factual Knowledge in Language Models
 4. Fast Model Editing at Scale
