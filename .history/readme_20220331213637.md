
# Paper Notes on Pretrain Language Models with Factual Knowledge


<!-- omit in toc -->
## Contents


- [Paper Notes on Pretrain Language Models with Factual Knowledge](#paper-notes-on-pretrain-language-models-with-factual-knowledge)
  - [Introduction](#introduction)
    - [Keywords Convention](#keywords-convention)
  - [Papers](#papers)
    - [Probing](#probing)
    - [Knowledge Retrieving](#knowledge-retrieving)
    - [Knowledge Injection](#knowledge-injection)
    - [Model Editing](#model-editing)
      - [Baseline](#baseline)
      - [Method](#method)
      - [Evaluation](#evaluation)
    - [Model Editing v.s. Continual Learning](#model-editing-vs-continual-learning)




## Introduction

This is a paper list about **PLMs & Knowledge**. 

### Keywords Convention

![](https://img.shields.io/badge/FaE-DCE7F1) abbreviation 

![](https://img.shields.io/badge/Efficient-EAD8D9) key features

![](https://img.shields.io/badge/QA-D8D0E1) main task

![](https://img.shields.io/badge/20220331-FAEFCA) editing info

## Papers


### Probing

1. LAMA
2. ParaREL

### Knowledge Retrieving

1. REALM

### Knowledge Injection

1. EaE
2. FaE
3. OPQL
   
### Model Editing
This section contains the pilot works that might contributes to the prevalence of model editiing paradigm.
1. **Editable Neural Networks** ICLR 2020. ![](https://img.shields.io/badge/Editable_training-DCE7F1) 

   *Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, Artem Babenko*.  [[pdf](https://openreview.net/pdf?id=HJedXaEtvS)], [[project](https://github.com/xtinkt/editable)],  2020.7
   ![](https://img.shields.io/badge/image_classification-D8D0E1) ![](https://img.shields.io/badge/machine_translation-D8D0E1)

   Related work:
   - re-train on the original dataset augmented with new samples (expensive)
   - use a manual cache (e.g. lookup table) that overrules model predictions on problematic samples (can't generalize to semantic equivalent inputs)
   Method:
   Editable Training (employ meta-learning techniques to ensure that mistakes can be corrected without harming its overall performance)
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/editable_train.png" width="800" alt="webhooks">


2. **Fact-based Text Editing** ACL 2020. ![](https://img.shields.io/badge/Fact_based_text_editing-DCE7F1) 
   
   *Hayate Iso, Chao Qiao, Hang Li*.  [[pdf](https://aclanthology.org/2020.acl-main.17.pdf)], [[project](https://github.com/isomap/factedit)],  2020.7
   ![](https://img.shields.io/badge/edit_draft_text_editing-D8D0E1) 


   Main cons.:
   - propose a new dataset (task): transforms a draft text into a revised text based on given triples
   - new model, FACTEDITOR. The model consists of three components, a buffer for storing the draft text and its representations, a stream for storing the revised text and its representations, and a memory for storing the triples and their representations
   <!-- ![Alt text-w15](editable_train.png) -->

   <img src="img/fte.png" width="300" alt="webhooks">


 2. Modifying Memories in Transformer Models
   Explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts.
    - updating stale knowledge
    - protecting privacy
    - eliminating unintended biases (debias)
  #### Baseline
    - Retraining the model on modified training set
    - Fine-tuning on modified facts
    - Fine-tuning on a mixture of modified and unmodified batches

  #### Method
    Constrained fine-tuning on supporting evidences for modified facts

 3. Editing Factual Knowledge in Language Models
   #### Evaluation
    success rate: updates the knowledge
    retain accuracy: retains the original predictions
    equivalence accuracy: for semantically equivalent inputs,
    performance deterioration: test performance deteriorates

 4. Fast Model Editing at Scale
    a model editor efficient: if the time and memory requirements for computing Ï† and evaluating E are small.
gradients are highdimensional objects ðŸ‘‰ using a low-rank decomposition of the gradient

### Model Editing v.s. Continual Learning
- Common: assimilating or updating a modelâ€™s behavior without catastrophic forgetting

Continual learning: learn a new task while preserving the performance on the previous tasks
wholly new behaviors or datasets
long sequences of model updates

Model editing: explicitly modifying specific factual knowledge in models while ensuring the model performance does not degrade on the unmodified facts
considers an edit or batch of edits applied all at once
requires the model to memorize new facts that conflict with previously learned facts


